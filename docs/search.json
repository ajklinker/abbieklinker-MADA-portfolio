[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Hi everyone! My name is Abbie Klinker and I am a second year master’s student in nutrition. I am currently researching food insecurity and food assistance around the state.\nWhile I’m currently studying nutrition I also have a background in statistics. I worked as a project lead in a data analysis consulting firm throughout undergrad and am excited to get back into the RStudio and data analysis space. I want to use this course to refresh my skills and work with some big data sets.\n\n\n\n\n\n\nMy fun fact(s)\nI did my undergrad at Miami of Ohio and am originally from Indiana. I am a washed-up runner and switched sports to join the UGA rowing team. I love being outside hiking, hammocking, and more!\n\n\n\nOur boat at nationals!\n\n\n\n\nData Analysis Links\nI saw this TikTok the other day and it pretty much explained my undergrad, especially with studying nutrition at the same time! Now working in research it has been super helpful to have that experience.\nhttps://www.tiktok.com/t/ZTRgaQxY9/"
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dslabs)\n\n#look at help file for gapminder data\n#help(gapminder)\n\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n#get better view of the data\nglimpse(gapminder)\n\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…"
  },
  {
    "objectID": "coding_exercise.html#examining-africa-data",
    "href": "coding_exercise.html#examining-africa-data",
    "title": "R Coding Exercise",
    "section": "Examining Africa Data",
    "text": "Examining Africa Data\n\nafricadata<- gapminder %>%\n  filter(continent == \"Africa\")\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\n\nInfant Mortality and Life Expectancy\n\nmort_expect <- africadata %>%\n  select(infant_mortality, life_expectancy, region) #kept region for sake of visualization\n\n#examine data \nstr(mort_expect)\n\n'data.frame':   2907 obs. of  3 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(mort_expect)\n\n infant_mortality life_expectancy                       region   \n Min.   : 11.40   Min.   :13.20   Eastern Africa           :912  \n 1st Qu.: 62.20   1st Qu.:48.23   Western Africa           :912  \n Median : 93.40   Median :53.98   Middle Africa            :456  \n Mean   : 95.12   Mean   :54.38   Northern Africa          :342  \n 3rd Qu.:124.70   3rd Qu.:60.10   Southern Africa          :285  \n Max.   :237.40   Max.   :77.60   Australia and New Zealand:  0  \n NA's   :226                      (Other)                  :  0  \n\n#Plot\nggplot()+\n  geom_point(aes(x=infant_mortality, y=life_expectancy, color=region), data=mort_expect)+ #kept region to more easily see trends since there's a lot of countries.\n  xlab(\"Infant Mortality Rate\")+ ylab(\"Life Expectancy (Yrs)\")+\n  theme_bw()\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nPopulation and Life Expectancy\n\npop_expect <- africadata %>%\n  select(population, life_expectancy, region) #kept region for same reason as prior plot\n\n#examine data \nstr(pop_expect)\n\n'data.frame':   2907 obs. of  3 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n $ region         : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(pop_expect)\n\n   population        life_expectancy                       region   \n Min.   :    41538   Min.   :13.20   Eastern Africa           :912  \n 1st Qu.:  1605232   1st Qu.:48.23   Western Africa           :912  \n Median :  5570982   Median :53.98   Middle Africa            :456  \n Mean   : 12235961   Mean   :54.38   Northern Africa          :342  \n 3rd Qu.: 13888152   3rd Qu.:60.10   Southern Africa          :285  \n Max.   :182201962   Max.   :77.60   Australia and New Zealand:  0  \n NA's   :51                          (Other)                  :  0  \n\n#Plot\nggplot()+\n  geom_point(aes(x=log(population), y=life_expectancy, color=region), data=pop_expect)+\n  theme_bw()+ xlab(\"Log Population\")+ ylab(\"Life Expectancy (Yrs)\")\n\nWarning: Removed 51 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "coding_exercise.html#select-years",
    "href": "coding_exercise.html#select-years",
    "title": "R Coding Exercise",
    "section": "Select Years",
    "text": "Select Years\nWhich years have the data missing for infant mortality?\n\nmort_expect_years <- africadata %>%\n  select(infant_mortality, life_expectancy, region, year)%>% #include years\n  filter(is.na(infant_mortality)) #look for missing values\n\nhead(mort_expect_years) #print\n\n  infant_mortality life_expectancy         region year\n1               NA           50.12 Western Africa 1960\n2               NA           40.95  Middle Africa 1960\n3               NA           45.77 Eastern Africa 1960\n4               NA           37.69  Middle Africa 1960\n5               NA           39.03 Eastern Africa 1960\n6               NA           38.83  Middle Africa 1960\n\n\n\nSelect Data for 2000\nFilter all Africa data for 2000\n\nafricadata2000<-africadata%>%\n  filter(year == 2000)\n\n#Double check code \nstr(africadata2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0"
  },
  {
    "objectID": "coding_exercise.html#quantify-the-data",
    "href": "coding_exercise.html#quantify-the-data",
    "title": "R Coding Exercise",
    "section": "Quantify the Data",
    "text": "Quantify the Data\nFitting infant mortality as a predictor of life expectancy\n\nfit1<- lm(life_expectancy ~ infant_mortality, data=africadata2000)\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      71.29331    2.42611  29.386  < 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nWe have evidence to support that infant mortality is a predictor of life expectancy. Based on the regression model, we can predict that for every unit increase in infant mortality, the average life expectancy decreases by 0.19 years (p=2.83e-8). The average predicted life expectancy with an infant mortality rate of 0 is 71.3 years.\nFitting population as a predictor of life expectancy\n\nfit2<- lm(population ~ infant_mortality, data=africadata2000)\nsummary(fit2)\n\n\nCall:\nlm(formula = population ~ infant_mortality, data = africadata2000)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-16307667 -12769228  -7828854    733380 105710100 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)      12063474    8682734   1.389    0.171\ninfant_mortality    45564     102671   0.444    0.659\n\nResidual standard error: 22260000 on 49 degrees of freedom\nMultiple R-squared:  0.004003,  Adjusted R-squared:  -0.01632 \nF-statistic: 0.1969 on 1 and 49 DF,  p-value: 0.6592\n\n\nSimilar to what we saw in the plots, we do not have evidence to say that population is associated with infant mortality (p=0.66).\nThis section added by Annabella Hines\nFirst, I wanted to look at the gapminder dataset as a whole specifically in the year 2000. I decided to create a boxplot to see the distributions of life expectancy for each continent.\n\n##create an object of the gapminder data for the year 2000\ncontinent<- gapminder %>% filter(year==2000)\n#A boxplot of the continent data viewing life expectancy by continent\nggplot(data=continent, aes(x=continent, y=life_expectancy, color=continent))+geom_boxplot()+xlab(\"Continent\")+ylab(\"Life Expectancy\")\n\n\n\n\nThe life expectancy distributions of each continent look fairly comparable except for Africa which has a lower overall distribution, lower median, and more outliers.\nNext, I compared fertility to infant mortality grouped by continent for the year 2000 to see if there were any noticeable trends.\n\n##created a scatterplot of fertility and infant mortality color coded by continent\nggplot(data=continent, aes(x=fertility, y=infant_mortality, color=continent))+geom_point()+ylab(\"Infant Mortality\")+ xlab(\"Fertility\")\n\nWarning: Removed 7 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThere seems to be a positive correlation between fertility and infant mortality, with Europe having low values in each and Africa having the highest.\nIn the next section I wanted to explore how the life expectancy changed across the years for the different regions in Africa.\n\n#create an object out of africadata with year, life expectancy, and region\nafricaregions<- africadata %>% select(year, life_expectancy, region)\n#create plot showing year vs. life expectancy color coded by region\nggplot(data=africaregions, aes(x=year, y=life_expectancy, color=region))+geom_smooth()+ylab(\"Life Expectancy\")+xlab(\"Year\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe plot shows a relatively positive correlation between year and life expectancy, so I wanted to run a fit to verify this observation.\n\n#Fit life expectanct against year for the africadata\nfit3<-lm(life_expectancy~year, data=africadata)\nsummary(fit3)\n\n\nCall:\nlm(formula = life_expectancy ~ year, data = africadata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.133  -5.197  -0.555   4.332  18.368 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -592.32739   16.12341  -36.74   <2e-16 ***\nyear           0.32531    0.00811   40.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.194 on 2905 degrees of freedom\nMultiple R-squared:  0.3564,    Adjusted R-squared:  0.3562 \nF-statistic:  1609 on 1 and 2905 DF,  p-value: < 2.2e-16\n\n\nAccording to the above data, year and life expectancy for the African countries are positively correlated at the 0.05 significance level.\n\n#Load broom and present lm output in a table\nlibrary(broom)\nmap_df(list(fit3), tidy)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) -592.     16.1         -36.7 5.24e-243\n2 year           0.325   0.00811      40.1 2.37e-280"
  },
  {
    "objectID": "dataanalysis-exercise.html",
    "href": "dataanalysis-exercise.html",
    "title": "Data Analysis Exercise - Smoking and Tobacco Use",
    "section": "",
    "text": "This data set is the Behavioral Risk Factor Surveillance System (CDC BRFSS) Smoking and Tobacco Use from 1995 to 2010. It includes the rate by which each state (and US territory) exhibits different smoking habits. The rates are weighted to population characteristics to allow for comparison of different population sizes. It includes variables such as “Smokes everyday” “Former Smoker” “Smokes some days” and “Never Smoked”. I chose it because it was complete and relatively clean, as well as spanned over a large amount of years. This allowed for more than 53 rows which I saw in multiple other data sets. I wanted to use something a bit larger than only having one entry per state.\nThis data has been used to create data visualizations also published on the CDC website if you’re interested in looking more into this data."
  },
  {
    "objectID": "dataanalysis-exercise.html#reading-in-the-data",
    "href": "dataanalysis-exercise.html#reading-in-the-data",
    "title": "Data Analysis Exercise - Smoking and Tobacco Use",
    "section": "Reading in the Data",
    "text": "Reading in the Data\nTobacco Use - Smoking Data for 1995-2010\nA Note I commented out the code chunk to only display the data table.\n\n\nRows: 876 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): State, Location 1\ndbl (5): Year, Smoke everyday, Smoke some days, Former smoker, Never smoked\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 10 × 7\n    Year State          `Smoke everyday` Smoke some da…¹ Forme…² Never…³ Locat…⁴\n   <dbl> <chr>                     <dbl>           <dbl>   <dbl>   <dbl> <chr>  \n 1  1996 Puerto Rico                 9.4             5.1    16      69.5 \"Puert…\n 2  2005 Virgin Islands              5.3             2.8    12.8    79.1 \"Virgi…\n 3  2005 Puerto Rico                 7.9             5.2    16.9    70   \"Puert…\n 4  2002 Virgin Islands              7               2.4    12.1    78.5 \"Virgi…\n 5  2003 Guam                       26.3             7.8    14.3    51.7  <NA>  \n 6  2000 Oregon                     15.3             5.4    28.3    50.9 \"Orego…\n 7  2002 New Mexico                 15               6.2    26      52.8 \"New M…\n 8  1996 Indiana                    24.9             3.7    23.7    47.6 \"India…\n 9  2006 Louisiana                  17.7             5.7    20.1    56.4 \"Louis…\n10  2002 Georgia                    17.7             5.6    22.4    54.3 \"Georg…\n# … with abbreviated variable names ¹​`Smoke some days`, ²​`Former smoker`,\n#   ³​`Never smoked`, ⁴​`Location 1`\n\n\nWe can see that there are two potential location variables, one which also includes latitude and longitude. I want to see if these locations are different, and separate the coordinates from the location. I’m also not sure what the coordinates mean – they may be the midpoint of the state, the capital, the data collection center, etc.\nA quick Google search for the first set of coordinates in Oregon showed that they point to a potential state midpoint. This is confirmed by also looking at those for Indiana (my hometown!) and being placed in the heart of downtown Indianapolis in the center of the state and those for Georgia and being placed in Macon.\nWe can also see 4 entries which are only coordinates for 2009-2010 Guam and Virgin Islands, so we need to make sure these get moved appropriately to the correct collumns when they are not structured the same way as the rest of the data."
  },
  {
    "objectID": "dataanalysis-exercise.html#cleaning-the-data",
    "href": "dataanalysis-exercise.html#cleaning-the-data",
    "title": "Data Analysis Exercise - Smoking and Tobacco Use",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\n\n#Separate location from coordinates\ntobacco_clean1<-separate(tobacco, col = `Location 1`, into = c(\"Location\", \"LatLong\"), sep = \"\\n\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 27 rows [1, 2,\n3, 4, 41, 42, 56, 121, 192, 204, 253, 293, 300, 366, 409, 424, 430, 447, 490,\n562, ...].\n\n\nThere are a couple warnings where there is no data for this field. I’m not super worried about those, I more just want to investigate and standardize the data that exists.\n\n#account for data with different structures\ntobacco_clean1$LatLong<-ifelse(tobacco_clean1$Year %in% c(2009, 2010) & \n                               tobacco_clean1$State %in% c(\"Virgin Islands\", \"Guam\"), \n                                  tobacco_clean1$Location,\n                                  tobacco_clean1$LatLong)\n\ntobacco_clean1$Location<-ifelse(tobacco_clean1$Year %in% c(2009, 2010) & \n                               tobacco_clean1$State %in% c(\"Virgin Islands\", \"Guam\"), \n                                 NA,\n                                 tobacco_clean1$Location)\n\n\n#separate from each other coordinates\ntobacco_clean2<-separate(tobacco_clean1, col = LatLong, into = c(\"Lat\", \"Long\"), sep = \",\")\ntobacco_clean2$Long<-str_sub(tobacco_clean2$Long, 1, str_length(tobacco_clean2$Long)-1)\ntobacco_clean2$Lat<-str_sub(tobacco_clean2$Lat, 2, -2)\n\n#look at similarities/differences in Location and State\ndifferent_locs<-tobacco_clean2%>%\n  filter(State != Location) \ndifferent_locs #none!\n\n# A tibble: 0 × 9\n# … with 9 variables: Year <dbl>, State <chr>, Smoke everyday <dbl>,\n#   Smoke some days <dbl>, Former smoker <dbl>, Never smoked <dbl>,\n#   Location <chr>, Lat <chr>, Long <chr>\n\n\nGreat! Since there are no unusual or different inputs we can remove one of the duplicate columns\n\ntobacco_clean_F<-tobacco_clean2%>%\n  select(-Location)"
  },
  {
    "objectID": "dataanalysis-exercise.html#wide-to-tall",
    "href": "dataanalysis-exercise.html#wide-to-tall",
    "title": "Data Analysis Exercise - Smoking and Tobacco Use",
    "section": "Wide to Tall",
    "text": "Wide to Tall\nNext, since each rate is its own column, this may make it difficult to analyze and compare, so I want to change it to wide-to-tall format.\n\ntobacco_tall<-gather(tobacco_clean_F, SmokeAmount, Rate, `Smoke everyday`:`Never smoked`)"
  },
  {
    "objectID": "dataanalysis-exercise.html#data-table-for-tobacco-use---smoking-data-for-1995-2010",
    "href": "dataanalysis-exercise.html#data-table-for-tobacco-use---smoking-data-for-1995-2010",
    "title": "Data Analysis Exercise - Smoking and Tobacco Use",
    "section": "Data Table for Tobacco Use - Smoking Data for 1995-2010",
    "text": "Data Table for Tobacco Use - Smoking Data for 1995-2010\n\n\n\n\n  \n\n\n\nThis will be our dataset for analysis! It allows us to group by the “SmokeAmount” variable while having a consistent variable of interest “Rate” among all categories."
  },
  {
    "objectID": "dataanalysis-exercise.html#kelly-hatfields-section",
    "href": "dataanalysis-exercise.html#kelly-hatfields-section",
    "title": "Data Analysis Exercise - Smoking and Tobacco Use",
    "section": "Kelly Hatfield’s Section",
    "text": "Kelly Hatfield’s Section\n\nStep 1: Viewing the Tobacco R data\n\nsummary(tobacco_tall)\n\n      Year         State                Lat             Long        \n Min.   :1995   Length:3504        Min.   :13.40   Min.   :-157.86  \n 1st Qu.:1999   Class :character   1st Qu.:35.47   1st Qu.:-106.13  \n Median :2003   Mode  :character   Median :39.49   Median : -89.54  \n Mean   :2003                      Mean   :39.60   Mean   : -92.63  \n 3rd Qu.:2007                      3rd Qu.:43.63   3rd Qu.: -78.46  \n Max.   :2010                      Max.   :64.85   Max.   : 144.78  \n                                   NA's   :240     NA's   :240      \n SmokeAmount             Rate       \n Length:3504        Min.   : 1.300  \n Class :character   1st Qu.: 7.275  \n Mode  :character   Median :21.000  \n                    Mean   :24.994  \n                    3rd Qu.:34.925  \n                    Max.   :83.700  \n                                    \n\n\n\n\nStep 2: See how many years and states are represented.\n\ntable(tobacco_tall$SmokeAmount)\n\n\n  Former smoker    Never smoked  Smoke everyday Smoke some days \n            876             876             876             876 \n\ntobacco_tall_NS = subset(tobacco_tall, SmokeAmount == \"Never smoked\")\ntobacco_tall_NS_2000=subset(tobacco_tall_NS, Year == 2000)\n\ntable(tobacco_tall_NS$State)\n\n\n                                 Alabama \n                                      16 \n                                  Alaska \n                                      16 \n                                 Arizona \n                                      16 \n                                Arkansas \n                                      16 \n                              California \n                                      16 \n                                Colorado \n                                      16 \n                             Connecticut \n                                      16 \n                                Delaware \n                                      16 \n                    District of Columbia \n                                      15 \n                                 Florida \n                                      16 \n                                 Georgia \n                                      16 \n                                    Guam \n                                       7 \n                                  Hawaii \n                                      15 \n                                   Idaho \n                                      16 \n                                Illinois \n                                      16 \n                                 Indiana \n                                      16 \n                                    Iowa \n                                      16 \n                                  Kansas \n                                      16 \n                                Kentucky \n                                      16 \n                               Louisiana \n                                      16 \n                                   Maine \n                                      16 \n                                Maryland \n                                      16 \n                           Massachusetts \n                                      16 \n                                Michigan \n                                      16 \n                               Minnesota \n                                      16 \n                             Mississippi \n                                      16 \n                                Missouri \n                                      16 \n                                 Montana \n                                      16 \n              Nationwide (States and DC) \n                                      16 \nNationwide (States, DC, and Territories) \n                                      16 \n                                Nebraska \n                                      16 \n                                  Nevada \n                                      16 \n                           New Hampshire \n                                      16 \n                              New Jersey \n                                      16 \n                              New Mexico \n                                      16 \n                                New York \n                                      16 \n                          North Carolina \n                                      16 \n                            North Dakota \n                                      16 \n                                    Ohio \n                                      16 \n                                Oklahoma \n                                      16 \n                                  Oregon \n                                      16 \n                            Pennsylvania \n                                      16 \n                             Puerto Rico \n                                      15 \n                            Rhode Island \n                                      16 \n                          South Carolina \n                                      16 \n                            South Dakota \n                                      16 \n                               Tennessee \n                                      16 \n                                   Texas \n                                      16 \n                                    Utah \n                                      14 \n                                 Vermont \n                                      16 \n                          Virgin Islands \n                                      10 \n                                Virginia \n                                      16 \n                              Washington \n                                      16 \n                           West Virginia \n                                      16 \n                               Wisconsin \n                                      16 \n                                 Wyoming \n                                      16 \n\ntable(tobacco_tall_NS$Year)\n\n\n1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 \n  51   53   54   54   54   54   56   56   56   54   55   55   56   56   56   56 \n\n\n\n\nStep 3: Create a table of average rate of Never Smokers by year\n\nresults <- aggregate(tobacco_tall_NS$Rate, list(tobacco_tall_NS$Year), FUN=mean)\n\nresults <- tobacco_tall_NS %>%\n  group_by(Year)%>%\n  summarise_at(vars(Rate), list('Average'=mean))\n#\nlibrary(knitr)\n\n\nknitr::kable(results, caption = \"Percent of Non-Smokers by State\", digits=2)\n\n\nPercent of Non-Smokers by State\n\n\nYear\nAverage\n\n\n\n\n1995\n52.08\n\n\n1996\n52.37\n\n\n1997\n52.98\n\n\n1998\n52.84\n\n\n1999\n53.39\n\n\n2000\n52.93\n\n\n2001\n52.67\n\n\n2002\n52.69\n\n\n2003\n53.04\n\n\n2004\n55.55\n\n\n2005\n54.87\n\n\n2006\n55.52\n\n\n2007\n55.83\n\n\n2008\n56.54\n\n\n2009\n56.89\n\n\n2010\n57.54\n\n\n\n\n#Change Never smoked variable name\n\n#\n\n\n\nStep 4: Create box plots and spaghetti plots showing rates of % Never smoked for states by year\n\n#Change Never smoked variable name\n\n\n\nggplot(tobacco_tall_NS, aes(x=factor(Year), y=Rate)) + geom_boxplot() + ylim(0,100)\n\n\n\nggplot(tobacco_tall_NS, aes(x=(Year), y=Rate, group=State)) + geom_line() + ylim(0,100)\n\n\n\n#\n\n\n\nStep 5: Print Top 5 States with highest percentage of never smokers in 2010\n\n#\nlibrary(dplyr)\n\nsorted_data <- subset(tobacco_tall_NS_2000,select=c(State,Year,Rate))\nsorted_data2 <- top_n(sorted_data,5,Rate) \nprint(sorted_data2)\n\n# A tibble: 6 × 3\n  State                 Year  Rate\n  <chr>                <dbl> <dbl>\n1 Arizona               2000  59.7\n2 Minnesota             2000  57.1\n3 Utah                  2000  68.8\n4 District of Columbia  2000  57.1\n5 Oklahoma              2000  57.4\n6 Puerto Rico           2000  71.6"
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "Exploration",
    "section": "",
    "text": "load(\"../../fluanalysis/data/clean_symptoms.rds\")"
  },
  {
    "objectID": "fluanalysis/code/exploration.html#dora-the-explorer",
    "href": "fluanalysis/code/exploration.html#dora-the-explorer",
    "title": "Exploration",
    "section": "Dora the Explorer!",
    "text": "Dora the Explorer!\nGoals: - For each (important) variable, produce and print some numerical output (e.g. a table or some summary statistics numbers).\n\nFor each (important) continuous variable, create a histogram or density plot.\nCreate scatterplots or boxplots or similar plots for the variable you decided is your main outcome of interest and the most important (or all depending on number of variables) independent variables/predictors. For this dataset, you can pick and choose a few predictor variables.\nAny other exploration steps that might be useful.\n\n\nContinuous Variable: Body Temperature\nLet’s jump in with our continuous variable first.\nSummary Table\n\nselect_sympact%>%   #need to include na.rm exception since we have those 5 pesky NAs\n  summarize(Avg = mean(BodyTemp, na.rm= T),\n            Min = min(BodyTemp, na.rm= T),\n            Max = max(BodyTemp, na.rm= T),\n            Range = Max-Min,\n            Q25 = quantile(BodyTemp, probs = .25, na.rm= T), \n            Median = median(BodyTemp, na.rm= T),\n            Q75 = quantile(BodyTemp, probs = .75, na.rm= T))\n\n       Avg  Min   Max Range  Q25 Median  Q75\n1 98.93507 97.2 103.1   5.9 98.2   98.5 99.3\n\n\nOff the bat, the good news is it doesn’t seem like we have any crazy outliers, as evidenced by the similarities between median and mean. Let’s make a graph to see if the 103 is an “outlier” (not large enough to be a true outlier, but maybe a lone wolf) or if there’s several participants with a fever > 100.\nSince we removed the unique visit number, we only have the one continuous variable, so this limits our possible EDA graph options.\nEDA Graphs: Body Temperature\n\nggplot()+\n  geom_bar(aes(x=BodyTemp), data=select_sympact, fill =\"#0072B2\")+    #I made it a pretty color for fun :)\n  theme_bw()\n\n\n\n\nSo we can see that having a temperature over 100 isn’t unusual. This is good to know moving forward. Are any of them considered true outliers?\n\nggplot()+\n  geom_boxplot(aes(BodyTemp), data=select_sympact, fill =\"#56B4E9\")+    #I made it a pretty color for fun :)\n  theme_bw()\n\n\n\n\nAlright this is good to know, even though a good number of people have temperatures greater than 101, by the spread available they are outliers in the data. Since there are so many and on a continuous scale, we won’t remove them, but these bad boys may be the reason we see the (slight) difference between median and mean.\nOff the top of my head when looking at possible predictors of body temperature, I would assume things like swollen lymph nodes, chills, subjective fever, myalgia, and weakness (Y/N) would be predictors. Let’s do some quick explorations with these variables.\n\nselect_sympact$Temp_group<-ifelse(between(select_sympact$BodyTemp , 97, 97.9), 97, \n                                  ifelse(between(select_sympact$BodyTemp , 98, 98.9), 98,\n                                         ifelse(between(select_sympact$BodyTemp , 99, 99.9), 99,\n                                                ifelse(between(select_sympact$BodyTemp , 100, 100.9), 100,\n                                                       ifelse(between(select_sympact$BodyTemp , 101, 101.9), 101,\n                                                              ifelse(between(select_sympact$BodyTemp , 102, 102.9), 102, 103))))))\n\ntable(select_sympact$Temp_group, select_sympact$SwollenLymphNodes)\n\n     \n       No Yes\n  97   48  44\n  98  223 167\n  99   81  55\n  100  31  23\n  101  18   9\n  102  12  10\n  103   5   4\n\ntable(select_sympact$Temp_group, select_sympact$ChillsSweats)\n\n     \n       No Yes\n  97   29  63\n  98   69 321\n  99   18 118\n  100   8  46\n  101   5  22\n  102   1  21\n  103   0   9\n\ntable(select_sympact$Temp_group, select_sympact$SubjectiveFever)\n\n     \n       No Yes\n  97   46  46\n  98  128 262\n  99   41  95\n  100   8  46\n  101   4  23\n  102   2  20\n  103   1   8\n\ntable(select_sympact$Temp_group, select_sympact$Weakness)\n\n     \n      None Mild Moderate Severe\n  97    12   27       38     15\n  98    25  130      182     53\n  99     7   37       69     23\n  100    3   11       26     14\n  101    1    7       10      9\n  102    1    9        9      3\n  103    0    2        4      3\n\ntable(select_sympact$Temp_group, select_sympact$Myalgia)\n\n     \n      None Mild Moderate Severe\n  97    16   23       41     12\n  98    44  114      178     54\n  99     8   44       56     28\n  100    8   11       25     10\n  101    1    8       12      6\n  102    2    9        8      3\n  103    0    4        5      0\n\n\nBased on the numbers alone, it seems like all of these variables are strong indicators of body temperature. The most questionable would be swollen lymph nodes. Checking these out in the analysis will definitely be helpful and should provide some solid models.\nMoving on…\nLet’s see what our categorical variable has in store for us.\n\n\nCategorical Variable: Nausea\nFirst, let’s see what our options for nausea entail:\n\nunique(select_sympact$Nausea)\n\n[1] No  Yes\nLevels: No Yes\n\n\nJust, yes and no, no maybe. This is good to know.\nHistorically, for “extreme” categories with no midline, I’ve used JMP to help with analysis, especially with factorial models. So this will be a learning experience for me. Off the bat, let’s compare general counts between the two options.\nSummary Table\n\nselect_sympact%>%   \n  count(Nausea)\n\n  Nausea   n\n1     No 477\n2    Yes 258\n\n\nEDA Graphs: Nausea\n\nggplot()+\n  geom_bar(aes(x=Nausea), data=select_sympact, fill =\"#0072B2\")+    \n  theme_bw()\n\n\n\n\nFor the benefit of the subjects, it’s nice to see most people aren’t experiencing nausea, though 258 cases isn’t something to ignore.\nBased on the symptoms available, I would assume that ab(domen) pain, chest pain, insomnia, vision, and vomit would be associated with nausea.\n\ntable(select_sympact$Nausea, select_sympact$AbPain)\n\n     \n       No Yes\n  No  446  31\n  Yes 196  62\n\ntable(select_sympact$Nausea, select_sympact$ChestPain)\n\n     \n       No Yes\n  No  342 135\n  Yes 159  99\n\ntable(select_sympact$Nausea, select_sympact$Insomnia)\n\n     \n       No Yes\n  No  215 262\n  Yes 101 157\n\ntable(select_sympact$Nausea, select_sympact$Vision)\n\n     \n       No Yes\n  No  468   9\n  Yes 248  10\n\ntable(select_sympact$Nausea, select_sympact$Vomit)\n\n     \n       No Yes\n  No  463  14\n  Yes 193  65\n\n\nBased on these contingency tables, the strongest evidence of a symptom of nausea is insomnia, but these will be further evaluated in the model as well.\n\n\nBoth?\nGreat, now we have a basic understanding of these symptoms independently, but can we look at them together or if they relate to one another?\nLet’s make a basic contingency table with temp grouped by each degree\n\ntable(select_sympact$Temp_group, select_sympact$Nausea)\n\n     \n       No Yes\n  97   67  25\n  98  248 142\n  99   90  46\n  100  33  21\n  101  16  11\n  102  18   4\n  103   3   6\n\n\nI couldn’t get the contingency table to give me conditional rates, so below are the calculated rates (%) of nausea among each temperature group:\n97: 27.2\n98: 36.4\n99: 33.8\n100: 38.9\n101: 40.7\n102: 18.2\n103: 66.7\n\nggplot()+\n  geom_bar(aes(x=BodyTemp, fill = Nausea), position = \"fill\", data=select_sympact)+    \n  theme_bw()\n\n\n\n\nI chose a “fill” position here because since there was such a dense amount of body temperatures around 98 degrees it was hard to see if there was a trend between nausea and body temperature. Once we standardize the chart to represent the ratio at each degree (group) we can see there may be a general increase of rates of nausea with higher body temperatures, but we should measure this further (quantitatively).\nWhen looking at the contingency tables, this is still a bit varied. The relationship may be something we look at more in the analysis, but for now, we have inconclusive results about the interrelation of nausea and body temperature."
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "Fitting",
    "section": "",
    "text": "load(\"../../fluanalysis/data/clean_symptoms.rds\")"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#model-fitting",
    "href": "fluanalysis/code/fitting.html#model-fitting",
    "title": "Fitting",
    "section": "Model Fitting",
    "text": "Model Fitting\nGoals:\n\nLoads cleaned data.\nFits a linear model to the continuous outcome (Body temperature) using only the main predictor of interest.\nFits another linear model to the continuous outcome using all (important) predictors of interest.\nCompares the model results for the model with just the main predictor and all predictors.\nFits a logistic model to the categorical outcome (Nausea) using only the main predictor of interest.\nFits another logistic model to the categorical outcome using all (important) predictors of interest.\nCompares the model results for the categorical model with just the main predictor and all predictors.\n\n\nContinuous + Runny Nose\n\nlm_fit <- \n  linear_reg()  %>% \n  set_engine(\"lm\")%>%\n  fit(BodyTemp ~ RunnyNose,  data = select_sympact)\ntidy(lm_fit)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0819   1210.   0      \n2 RunnyNoseYes   -0.293    0.0971     -3.01 0.00268\n\n\n\n\nContinuous + Everything\nSwollen lymph nodes, chills, subjective fever, myalgia, and weakness (Y/N)\n\nlm_fit2 <- \n  linear_reg()  %>% \n  set_engine(\"lm\") %>%\n  fit(BodyTemp ~ SwollenLymphNodes + ChillsSweats +   SubjectiveFever + MyalgiaYN +  Weakness,  data = select_sympact)\ntidy(lm_fit2)\n\n# A tibble: 8 × 5\n  term                 estimate std.error statistic   p.value\n  <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)           98.4       0.197    499.    0        \n2 SwollenLymphNodesYes  -0.120     0.0880    -1.36  0.174    \n3 ChillsSweatsYes        0.157     0.126      1.24  0.214    \n4 SubjectiveFeverYes     0.443     0.102      4.34  0.0000164\n5 MyalgiaYNYes           0.0748    0.152      0.491 0.623    \n6 WeaknessMild           0.107     0.190      0.562 0.574    \n7 WeaknessModerate       0.0972    0.191      0.510 0.610    \n8 WeaknessSevere         0.335     0.212      1.57  0.116    \n\n\n\n\nCategorical + Runny Nose\n\nlm_fit3 <- \n  logistic_reg()  %>% \n  set_engine(\"glm\") %>%\n  fit(Nausea ~ RunnyNose,  data = select_sympact)\ntidy(lm_fit3)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic    p.value\n  <chr>           <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   -0.658      0.145    -4.53  0.00000589\n2 RunnyNoseYes   0.0605     0.172     0.353 0.724     \n\n\n\n\nCategorical + Everything\nAbdomen pain, chest pain, insomnia, vision, and vomit\n\nlm_fit4 <- \n  logistic_reg()  %>% \n  set_engine(\"glm\") %>%\n  fit(Nausea ~ AbPain + ChestPain +   Insomnia + Vision +  Vomit,  data = select_sympact)\ntidy(lm_fit4)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    -1.21      0.145    -8.36  6.22e-17\n2 AbPainYes       1.24      0.255     4.87  1.09e- 6\n3 ChestPainYes    0.170     0.183     0.928 3.53e- 1\n4 InsomniaYes     0.193     0.172     1.12  2.61e- 1\n5 VisionYes       0.588     0.510     1.15  2.49e- 1\n6 VomitYes        2.27      0.314     7.24  4.57e-13\n\n\n\n\nComparisons\nIn terms of both of our outcomes of interest, using a more complex model may be more advantageous, especially considering the dataset has 32 variables of interest. It is limited to say that one variable has stronger predictive qualities than multiple used jointly (although chosen out of the statistician’s (me) biases).\nOne drawback of using multiple variables is that in both categorical and continuous models, the predictors were split between yes/no and likert scales. For more in-depth analyses these should be considered more in depth - either by only using the yes/no scales, or finding a way to moderate the effect of different levels of factors between the different predictors. Applying the models’ findings within the predict() function would also highlight drawbacks between the different models, and it’s great that tidymodels has this capability built in rather than going through 5 extra steps using individual model packages. I hate to say that I’m not able to try it out at the moment due to this excercise’s deadline.\nI would love to play around with tidymodels more as well as KNN and Bayesian models. Exploring more of the tidymodels output as well as different model types is super interesting to me. I think I’m beginning to understand the tidymodels framework, and I’m intrigued by the implications it has outside of regression and logistical models.\nUnfortunately due to the nature of deadlines, being able to take advantage of this exercise in its entirety hasn’t been possible, but over the next few weeks I’m very interested in learning more about machine learning and its methods through R. Over spring break I’ll likely revisit this exercise to better understand tidymodels and its capabilities - both for this class as well as my own research. If I discover any new insights I can add an addendum to this code.\nThank you!"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "Model Evaluation",
    "section": "",
    "text": "Read in previously cleaned data\n\nload(\"../../fluanalysis/data/clean_symptoms.rds\")\n\nTrain and test:\n\nset.seed(1234) \n# Since we have less than 1000 observations, I'm going to do a 50/50 split.\ndata_split <- initial_split(select_sympact, prop = 1/2)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#fit-a-model-with-workflow",
    "href": "fluanalysis/code/modeleval.html#fit-a-model-with-workflow",
    "title": "Model Evaluation",
    "section": "Fit a Model with Workflow",
    "text": "Fit a Model with Workflow\n\ncat_model1 <- \n  logistic_reg()  %>% \n  set_engine(\"glm\") \n\ncat1_wflow <- \n  workflow() %>% \n  add_model(cat_model1) %>% \n  add_recipe(cat_recipe1)\n\ncat1_fit <- \n  cat1_wflow %>% \n  fit(data = train_data)\n\ncat1_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)          -7.72       10.9     -0.706   0.480 \n 2 SwollenLymphNodesYes -0.0254      0.288   -0.0880  0.930 \n 3 ChestCongestionYes    0.639       0.326    1.96    0.0498\n 4 ChillsSweatsYes       0.545       0.450    1.21    0.226 \n 5 NasalCongestionYes    0.297       0.362    0.821   0.412 \n 6 CoughYNYes           -0.446       0.710   -0.628   0.530 \n 7 SneezeYes             0.336       0.316    1.07    0.287 \n 8 FatigueYes            0.185       0.541    0.342   0.732 \n 9 SubjectiveFeverYes   -0.00883     0.345   -0.0256  0.980 \n10 HeadacheYes          -0.0243      0.402   -0.0604  0.952 \n# … with 28 more rows\n\n\nCheck with testing data\n\npredict(cat1_fit, test_data)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 368 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 Yes        \n 4 No         \n 5 Yes        \n 6 No         \n 7 No         \n 8 No         \n 9 Yes        \n10 No         \n# … with 358 more rows\n\ncat1_aug <- \n  augment(cat1_fit, test_data)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\ncat1_aug\n\n# A tibble: 368 × 35\n   SwollenLymph…¹ Chest…² Chill…³ Nasal…⁴ CoughYN Sneeze Fatigue Subje…⁵ Heada…⁶\n   <fct>          <fct>   <fct>   <fct>   <fct>   <fct>  <fct>   <fct>   <fct>  \n 1 Yes            No      No      No      Yes     No     Yes     Yes     Yes    \n 2 Yes            Yes     No      Yes     Yes     No     Yes     Yes     Yes    \n 3 Yes            Yes     Yes     Yes     No      Yes    Yes     Yes     Yes    \n 4 Yes            No      Yes     No      No      No     Yes     Yes     Yes    \n 5 No             No      Yes     No      Yes     Yes    Yes     Yes     Yes    \n 6 No             No      Yes     No      Yes     No     Yes     Yes     No     \n 7 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n 8 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n 9 Yes            Yes     Yes     Yes     No      No     Yes     Yes     Yes    \n10 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n# … with 358 more rows, 26 more variables: Weakness <fct>, WeaknessYN <fct>,\n#   CoughIntensity <fct>, CoughYN2 <fct>, Myalgia <fct>, MyalgiaYN <fct>,\n#   RunnyNose <fct>, AbPain <fct>, ChestPain <fct>, Diarrhea <fct>,\n#   EyePn <fct>, Insomnia <fct>, ItchyEye <fct>, Nausea <fct>, EarPn <fct>,\n#   Hearing <fct>, Pharyngitis <fct>, Breathless <fct>, ToothPn <fct>,\n#   Vision <fct>, Vomit <fct>, Wheeze <fct>, BodyTemp <dbl>, .pred_class <fct>,\n#   .pred_No <dbl>, .pred_Yes <dbl>, and abbreviated variable names …"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#roc---auc",
    "href": "fluanalysis/code/modeleval.html#roc---auc",
    "title": "Model Evaluation",
    "section": "ROC - AUC",
    "text": "ROC - AUC\nTraining Data\n\ncat1_train <- \n  augment(cat1_fit, train_data)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\ncat1_train\n\n# A tibble: 367 × 35\n   SwollenLymph…¹ Chest…² Chill…³ Nasal…⁴ CoughYN Sneeze Fatigue Subje…⁵ Heada…⁶\n   <fct>          <fct>   <fct>   <fct>   <fct>   <fct>  <fct>   <fct>   <fct>  \n 1 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n 2 No             Yes     Yes     No      Yes     No     Yes     No      Yes    \n 3 No             No      Yes     No      Yes     Yes    Yes     No      Yes    \n 4 No             No      Yes     Yes     Yes     No     Yes     No      Yes    \n 5 Yes            Yes     Yes     No      Yes     No     Yes     Yes     Yes    \n 6 Yes            No      No      Yes     Yes     Yes    Yes     No      Yes    \n 7 Yes            No      No      Yes     Yes     No     No      No      No     \n 8 No             No      Yes     No      No      No     Yes     No      Yes    \n 9 Yes            No      Yes     Yes     Yes     Yes    Yes     No      No     \n10 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n# … with 357 more rows, 26 more variables: Weakness <fct>, WeaknessYN <fct>,\n#   CoughIntensity <fct>, CoughYN2 <fct>, Myalgia <fct>, MyalgiaYN <fct>,\n#   RunnyNose <fct>, AbPain <fct>, ChestPain <fct>, Diarrhea <fct>,\n#   EyePn <fct>, Insomnia <fct>, ItchyEye <fct>, Nausea <fct>, EarPn <fct>,\n#   Hearing <fct>, Pharyngitis <fct>, Breathless <fct>, ToothPn <fct>,\n#   Vision <fct>, Vomit <fct>, Wheeze <fct>, BodyTemp <dbl>, .pred_class <fct>,\n#   .pred_No <dbl>, .pred_Yes <dbl>, and abbreviated variable names …\n\ncat1_train %>% \n  roc_curve(truth = Nausea, .pred_No) %>%  #looked at the percent predicted NO Nausea rather than Yes to see the \"under the curve\" versus .pred_Yes providing area \"over\" the curve\n  autoplot()\n\n\n\ncat1_train %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.786\n\n\nTesting Data\n\ncat1_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>%  #looked at the percent predicted NO Nausea rather than Yes to see the \"under the curve\" versus .pred_Yes providing area \"over\" the curve\n  autoplot()\n\n\n\ncat1_aug %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.726\n\n\nWe see that the training data performed a bit better with our augment ROC-AUC estimated to be 0.79 versus with the tested data at 0.73, but both are still < 0.70 which is a promising start."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#fit-a-model-with-workflow-1",
    "href": "fluanalysis/code/modeleval.html#fit-a-model-with-workflow-1",
    "title": "Model Evaluation",
    "section": "Fit a Model with Workflow",
    "text": "Fit a Model with Workflow\n\ncat_model2 <- \n  logistic_reg()  %>% \n  set_engine(\"glm\") \n\ncat2_wflow <- \n  workflow() %>% \n  add_model(cat_model2) %>% \n  add_recipe(cat_recipe2)\n\ncat2_fit <- \n  cat2_wflow %>% \n  fit(data = train_data)\n\ncat2_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -0.627      0.198    -3.16  0.00157\n2 RunnyNoseYes  -0.0312     0.238    -0.131 0.896  \n\n\nCheck with testing data\n\npredict(cat2_fit, test_data)\n\n# A tibble: 368 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 358 more rows\n\ncat2_aug <- \n  augment(cat2_fit, test_data)\n\ncat2_aug\n\n# A tibble: 368 × 35\n   SwollenLymph…¹ Chest…² Chill…³ Nasal…⁴ CoughYN Sneeze Fatigue Subje…⁵ Heada…⁶\n   <fct>          <fct>   <fct>   <fct>   <fct>   <fct>  <fct>   <fct>   <fct>  \n 1 Yes            No      No      No      Yes     No     Yes     Yes     Yes    \n 2 Yes            Yes     No      Yes     Yes     No     Yes     Yes     Yes    \n 3 Yes            Yes     Yes     Yes     No      Yes    Yes     Yes     Yes    \n 4 Yes            No      Yes     No      No      No     Yes     Yes     Yes    \n 5 No             No      Yes     No      Yes     Yes    Yes     Yes     Yes    \n 6 No             No      Yes     No      Yes     No     Yes     Yes     No     \n 7 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n 8 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n 9 Yes            Yes     Yes     Yes     No      No     Yes     Yes     Yes    \n10 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n# … with 358 more rows, 26 more variables: Weakness <fct>, WeaknessYN <fct>,\n#   CoughIntensity <fct>, CoughYN2 <fct>, Myalgia <fct>, MyalgiaYN <fct>,\n#   RunnyNose <fct>, AbPain <fct>, ChestPain <fct>, Diarrhea <fct>,\n#   EyePn <fct>, Insomnia <fct>, ItchyEye <fct>, Nausea <fct>, EarPn <fct>,\n#   Hearing <fct>, Pharyngitis <fct>, Breathless <fct>, ToothPn <fct>,\n#   Vision <fct>, Vomit <fct>, Wheeze <fct>, BodyTemp <dbl>, .pred_class <fct>,\n#   .pred_No <dbl>, .pred_Yes <dbl>, and abbreviated variable names …"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#roc---auc-1",
    "href": "fluanalysis/code/modeleval.html#roc---auc-1",
    "title": "Model Evaluation",
    "section": "ROC - AUC",
    "text": "ROC - AUC\nTraining Data\n\ncat2_train <- \n  augment(cat2_fit, train_data)\n\ncat2_train\n\n# A tibble: 367 × 35\n   SwollenLymph…¹ Chest…² Chill…³ Nasal…⁴ CoughYN Sneeze Fatigue Subje…⁵ Heada…⁶\n   <fct>          <fct>   <fct>   <fct>   <fct>   <fct>  <fct>   <fct>   <fct>  \n 1 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n 2 No             Yes     Yes     No      Yes     No     Yes     No      Yes    \n 3 No             No      Yes     No      Yes     Yes    Yes     No      Yes    \n 4 No             No      Yes     Yes     Yes     No     Yes     No      Yes    \n 5 Yes            Yes     Yes     No      Yes     No     Yes     Yes     Yes    \n 6 Yes            No      No      Yes     Yes     Yes    Yes     No      Yes    \n 7 Yes            No      No      Yes     Yes     No     No      No      No     \n 8 No             No      Yes     No      No      No     Yes     No      Yes    \n 9 Yes            No      Yes     Yes     Yes     Yes    Yes     No      No     \n10 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n# … with 357 more rows, 26 more variables: Weakness <fct>, WeaknessYN <fct>,\n#   CoughIntensity <fct>, CoughYN2 <fct>, Myalgia <fct>, MyalgiaYN <fct>,\n#   RunnyNose <fct>, AbPain <fct>, ChestPain <fct>, Diarrhea <fct>,\n#   EyePn <fct>, Insomnia <fct>, ItchyEye <fct>, Nausea <fct>, EarPn <fct>,\n#   Hearing <fct>, Pharyngitis <fct>, Breathless <fct>, ToothPn <fct>,\n#   Vision <fct>, Vomit <fct>, Wheeze <fct>, BodyTemp <dbl>, .pred_class <fct>,\n#   .pred_No <dbl>, .pred_Yes <dbl>, and abbreviated variable names …\n\ncat2_train %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\n\n\ncat2_train %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.503\n\n\nTesting Data\n\ncat2_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>%  \n  autoplot()\n\n\n\ncat2_aug %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.485\n\n\nHere we see a similar difference between the train/test models with a difference of about 0.2 in the ROC-AUC estimates; however, overall type of model performs a lot worse than the predictive power of that with all the variables included. This makes sense as there are many types of symptoms associated in different componations with different illnesses, so accounting for these especially for an illness as general as the flu may be more advantageous in predicting the patient’s symptoms/experience."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#fitting-the-above-steps-with-a-continuous-outcome-body-temperature-and-all-preditors",
    "href": "fluanalysis/code/modeleval.html#fitting-the-above-steps-with-a-continuous-outcome-body-temperature-and-all-preditors",
    "title": "Model Evaluation",
    "section": "Fitting the above steps with a continuous outcome (Body Temperature) and all preditors",
    "text": "Fitting the above steps with a continuous outcome (Body Temperature) and all preditors\n\nflu_mod10_cont_rec <- recipe(BodyTemp ~ ., data = train_data) \n\n\nWorkflow creation and model fitting\n\n# Linear regression \ncont_model1 <- \n  linear_reg()  %>% \n  set_engine(\"lm\") \n\n# Paring model with receip\ncont1_wflow <- \n  workflow() %>% \n  add_model(cont_model1) %>% \n  add_recipe(flu_mod10_cont_rec)\n\n# Looking at the model output\ncont1_fit <- \n  cont1_wflow %>% \n  fit(data = train_data)\ncont1_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)           97.8        0.451   217.    0      \n 2 SwollenLymphNodesYes  -0.406      0.137    -2.96  0.00328\n 3 ChestCongestionYes     0.116      0.150     0.774 0.440  \n 4 ChillsSweatsYes       -0.0681     0.200    -0.340 0.734  \n 5 NasalCongestionYes    -0.0587     0.168    -0.350 0.726  \n 6 CoughYNYes             0.201      0.322     0.626 0.532  \n 7 SneezeYes             -0.292      0.149    -1.95  0.0515 \n 8 FatigueYes             0.329      0.236     1.40  0.164  \n 9 SubjectiveFeverYes     0.532      0.162     3.29  0.00111\n10 HeadacheYes            0.114      0.186     0.614 0.540  \n# … with 28 more rows\n\n\n\n\nUsing the trained workflow (cont1_fit) to predict with the unseen test data\n\npredict(cont1_fit, test_data)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 368 × 1\n   .pred\n   <dbl>\n 1  99.2\n 2  99.3\n 3  98.9\n 4  99.0\n 5  99.4\n 6  99.6\n 7  98.7\n 8  98.5\n 9  99.1\n10  98.7\n# … with 358 more rows\n\ncont1_aug <- \n  augment(cont1_fit, test_data)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\ncont1_aug\n\n# A tibble: 368 × 33\n   SwollenLymph…¹ Chest…² Chill…³ Nasal…⁴ CoughYN Sneeze Fatigue Subje…⁵ Heada…⁶\n   <fct>          <fct>   <fct>   <fct>   <fct>   <fct>  <fct>   <fct>   <fct>  \n 1 Yes            No      No      No      Yes     No     Yes     Yes     Yes    \n 2 Yes            Yes     No      Yes     Yes     No     Yes     Yes     Yes    \n 3 Yes            Yes     Yes     Yes     No      Yes    Yes     Yes     Yes    \n 4 Yes            No      Yes     No      No      No     Yes     Yes     Yes    \n 5 No             No      Yes     No      Yes     Yes    Yes     Yes     Yes    \n 6 No             No      Yes     No      Yes     No     Yes     Yes     No     \n 7 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n 8 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n 9 Yes            Yes     Yes     Yes     No      No     Yes     Yes     Yes    \n10 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n# … with 358 more rows, 24 more variables: Weakness <fct>, WeaknessYN <fct>,\n#   CoughIntensity <fct>, CoughYN2 <fct>, Myalgia <fct>, MyalgiaYN <fct>,\n#   RunnyNose <fct>, AbPain <fct>, ChestPain <fct>, Diarrhea <fct>,\n#   EyePn <fct>, Insomnia <fct>, ItchyEye <fct>, Nausea <fct>, EarPn <fct>,\n#   Hearing <fct>, Pharyngitis <fct>, Breathless <fct>, ToothPn <fct>,\n#   Vision <fct>, Vomit <fct>, Wheeze <fct>, BodyTemp <dbl>, .pred <dbl>, and\n#   abbreviated variable names ¹​SwollenLymphNodes, ²​ChestCongestion, …\n\n\n\n\nRMSE\n\nStarting with Training data\n\ncont1_train <- \n  augment(cont1_fit, train_data)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\nyardstick::rmse(cont1_train, BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n\n\nRMSE of training model is around 1.13\n\n\nNow Testing data\n\nyardstick::rmse(cont1_aug, BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.19\n\n\nRMSE of testing model is around 1.15\nThus, the training model perfomence is slighly better than the testing, but that difference is very minimal (1.15 - 1.13 = 0.02)."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#continuous-with-runnynose-as-the-main-predictor",
    "href": "fluanalysis/code/modeleval.html#continuous-with-runnynose-as-the-main-predictor",
    "title": "Model Evaluation",
    "section": "Continuous with RunnyNose as the main predictor",
    "text": "Continuous with RunnyNose as the main predictor\n\ncont_recipe2 <- \n  recipe(BodyTemp ~ RunnyNose, data = train_data) \n\n\nFit a Model with Workflow\n\n# Setting up a linear model\ncont_model2 <- \n  linear_reg()  %>% \n  set_engine(\"lm\") \n\n# Modeling workflow for pairing model and recipe\ncont2_wflow <- \n  workflow() %>% \n  add_model(cont_model2) %>% \n  add_recipe(cont_recipe2)\n\n# Using the resulting predictors for preparing recipe and training model\ncont2_fit <- \n  cont2_wflow %>% \n  fit(data = train_data)\n\n# Pulling the fitted model object and using tidy() function for getting a tidy tibble of model coefficients. \ncont2_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1       0.116    852.     0    \n2 RunnyNoseYes   -0.187     0.140     -1.33   0.183\n\n\n\n\nRMSE\n\nStarting with Trainind data\n\ncont2_train <- \n  augment(cont2_fit, train_data)\nyardstick::rmse(cont2_train, BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.23\n\n\nEstimated RMSE is 1.23\n\n\nNow using the testing data\n\npredict(cont2_fit, test_data)\n\n# A tibble: 368 × 1\n   .pred\n   <dbl>\n 1  99.1\n 2  99.1\n 3  98.9\n 4  99.1\n 5  99.1\n 6  98.9\n 7  98.9\n 8  98.9\n 9  99.1\n10  98.9\n# … with 358 more rows\n\ncont2_aug <- \n  augment(cont2_fit, test_data)\ncont2_aug\n\n# A tibble: 368 × 33\n   SwollenLymph…¹ Chest…² Chill…³ Nasal…⁴ CoughYN Sneeze Fatigue Subje…⁵ Heada…⁶\n   <fct>          <fct>   <fct>   <fct>   <fct>   <fct>  <fct>   <fct>   <fct>  \n 1 Yes            No      No      No      Yes     No     Yes     Yes     Yes    \n 2 Yes            Yes     No      Yes     Yes     No     Yes     Yes     Yes    \n 3 Yes            Yes     Yes     Yes     No      Yes    Yes     Yes     Yes    \n 4 Yes            No      Yes     No      No      No     Yes     Yes     Yes    \n 5 No             No      Yes     No      Yes     Yes    Yes     Yes     Yes    \n 6 No             No      Yes     No      Yes     No     Yes     Yes     No     \n 7 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n 8 Yes            Yes     Yes     Yes     Yes     No     Yes     Yes     Yes    \n 9 Yes            Yes     Yes     Yes     No      No     Yes     Yes     Yes    \n10 No             Yes     Yes     Yes     Yes     Yes    Yes     Yes     Yes    \n# … with 358 more rows, 24 more variables: Weakness <fct>, WeaknessYN <fct>,\n#   CoughIntensity <fct>, CoughYN2 <fct>, Myalgia <fct>, MyalgiaYN <fct>,\n#   RunnyNose <fct>, AbPain <fct>, ChestPain <fct>, Diarrhea <fct>,\n#   EyePn <fct>, Insomnia <fct>, ItchyEye <fct>, Nausea <fct>, EarPn <fct>,\n#   Hearing <fct>, Pharyngitis <fct>, Breathless <fct>, ToothPn <fct>,\n#   Vision <fct>, Vomit <fct>, Wheeze <fct>, BodyTemp <dbl>, .pred <dbl>, and\n#   abbreviated variable names ¹​SwollenLymphNodes, ²​ChestCongestion, …\n\n## RMSE - TEST\nyardstick::rmse(cont2_aug, BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.15\n\n\nEstimated RMSE is 1.15.\nThus the testing model performs better than the training model."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "I was having some difficulties “loading” the code as mentioned in the description for this exercise due to a “magic number X,” but readRDS function seemed ok!\n\nSympAct_Any_Pos<-readRDS(\"../../fluanalysis/data/SympAct_Any_Pos.Rda\")"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#clean-the-code",
    "href": "fluanalysis/code/wrangling.html#clean-the-code",
    "title": "Wrangling",
    "section": "Clean the Code",
    "text": "Clean the Code\nAlrighty, now it’s time to clean this up for the exercise. The first step is to remove all variables that have Score or Total or FluA or FluB or DxName or Activity in their name\n\nselect_sympact<-SympAct_Any_Pos%>%\n  select(-c(contains(\"Score\"),contains(\"Total\"), contains(\"FluA\"), contains(\"FluB\"), contains(\"DxName\"), contains(\"Activity\"), Unique.Visit))\n\n\n\n\n\n  \n\n\n\nGreat, this takes us down to 32 variables from 63, each with a list of symptoms the pt may or may not be experiencing. Now we have a lovely dataset we can use for future analysis.\nSince we are looking at Body Temperature and Nausea as our main outcome variables, let’s investigate them quickly and make sure they are good to go for our next exploratory and analysis steps.\n\nselect_sympact%>%\n  filter(is.na(BodyTemp))\n\n  SwollenLymphNodes ChestCongestion ChillsSweats NasalCongestion CoughYN Sneeze\n1                No             Yes          Yes             Yes     Yes    Yes\n2               Yes             Yes          Yes              No     Yes    Yes\n3                No              No          Yes              No     Yes    Yes\n4               Yes              No           No             Yes     Yes    Yes\n5                No              No          Yes              No     Yes     No\n  Fatigue SubjectiveFever Headache Weakness WeaknessYN CoughIntensity CoughYN2\n1     Yes             Yes      Yes Moderate        Yes       Moderate      Yes\n2     Yes             Yes      Yes Moderate        Yes       Moderate      Yes\n3     Yes             Yes      Yes   Severe        Yes       Moderate      Yes\n4     Yes             Yes      Yes     Mild        Yes           Mild      Yes\n5     Yes             Yes      Yes Moderate        Yes           Mild      Yes\n   Myalgia MyalgiaYN RunnyNose AbPain ChestPain Diarrhea EyePn Insomnia\n1 Moderate       Yes       Yes     No        No       No    No      Yes\n2   Severe       Yes       Yes    Yes       Yes       No    No      Yes\n3   Severe       Yes       Yes    Yes        No       No    No       No\n4     Mild       Yes       Yes     No        No       No    No      Yes\n5 Moderate       Yes       Yes     No        No       No    No      Yes\n  ItchyEye Nausea EarPn Hearing Pharyngitis Breathless ToothPn Vision Vomit\n1       No     No    No      No          No        Yes      No     No    No\n2      Yes    Yes    No      No         Yes        Yes      No     No    No\n3       No    Yes    No      No         Yes         No      No     No    No\n4      Yes     No    No      No         Yes         No      No     No    No\n5      Yes    Yes    No      No          No        Yes     Yes     No   Yes\n  Wheeze BodyTemp\n1    Yes       NA\n2     No       NA\n3     No       NA\n4     No       NA\n5     No       NA\n\nselect_sympact%>%\n  filter(is.na(Nausea))\n\n [1] SwollenLymphNodes ChestCongestion   ChillsSweats      NasalCongestion  \n [5] CoughYN           Sneeze            Fatigue           SubjectiveFever  \n [9] Headache          Weakness          WeaknessYN        CoughIntensity   \n[13] CoughYN2          Myalgia           MyalgiaYN         RunnyNose        \n[17] AbPain            ChestPain         Diarrhea          EyePn            \n[21] Insomnia          ItchyEye          Nausea            EarPn            \n[25] Hearing           Pharyngitis       Breathless        ToothPn          \n[29] Vision            Vomit             Wheeze            BodyTemp         \n<0 rows> (or 0-length row.names)\n\n\nTo be aware of in the future, we have five missing values for BodyTemp, but none for Nausea. This is good to know as we dive into EDA and model fitting. Overall, this seems like a pretty clean dataset, so let’s export it and get into the juicy stuff.\n\nsave(select_sympact, file = \"../../fluanalysis/data/clean_symptoms.rds\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abbie’s Website and Data Analysis Portfolio",
    "section": "",
    "text": "Hello\n\nAnd hello again\nWelcome to my website and data analysis portfolio.\nI’m excited to get started with this class!\n\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "This is my first Tidy Tuesday exercise! I feel like this is such a cool community to be a part of, and I’m excited to get into it.\nFirst thing’s first, let’s load in the data."
  },
  {
    "objectID": "tidytuesday_exercise.html#data-exploration",
    "href": "tidytuesday_exercise.html#data-exploration",
    "title": "Tidy Tuesday Exercise",
    "section": "Data Exploration",
    "text": "Data Exploration\nAlrighty, off the bat it looks like the first actor is usually a guy while the second one is a mix of men and women, but I want to check this out.\n\nunique(movies$Actor.1.Gender) #There's both! How many of each?\n\n[1] \"man\"   \"woman\"\n\nsum(movies$Actor.1.Gender == \"man\") #1139\n\n[1] 1139\n\nsum(movies$Actor.1.Gender == \"woman\") #16\n\n[1] 16\n\nunique(movies$Actor.2.Gender) \n\n[1] \"woman\" \"man\"  \n\nsum(movies$Actor.2.Gender == \"man\") #17\n\n[1] 17\n\nsum(movies$Actor.2.Gender == \"woman\") #1138\n\n[1] 1138\n\n\nDo we have any overlap in the few where men/women are flipped?\n\nmovies %>%\n  filter(Actor.1.Name %in% Actor.2.Name)%>%\n  distinct(Actor.1.Name)\n\n           Actor.1.Name\n1        Julianne Moore\n2         Ralph Fiennes\n3          Daniel Craig\n4        Cate Blanchett\n5          James Franco\n6            Matt Damon\n7         Ewan McGregor\n8         Matthew Goode\n9           Léa Seydoux\n10        Russell Brand\n11      Charlize Theron\n12         Rebecca Hall\n13           Taye Diggs\n14          Lena Headey\n15         Heath Ledger\n16 Kristin Scott Thomas\n17       Annette Bening\n18    Timothée Chalamet\n19       Nicholas Hoult\n\n\nOk so we have 19 actors that are in both Actor.1 and Actor.2. Right now we might not need to adjust for this, but it’s good to know for the future.\nSo there seems to be a flip-flop of leading men/ladies, and about 1 of each gender per film. So, the order of Actor 1 and Actor 2 it’s not exclusively men and women, is Actor 1 the older one?\n\nmovies%>%\n  mutate(act1.agediff = Actor.1.Age - Actor.2.Age)%>%\n  count(act1.agediff < 0)\n\n  act1.agediff < 0   n\n1            FALSE 969\n2             TRUE 186\n\n\nWe have 186 instances where Actor 2 is older than Actor 1. It seems like for this dataset is a bit arbitrary in terms of who is listed 1st and 2nd - unless it’s by whoever is paid most which is information we don’t have here.\nWelp, we’ll figure out what to do with this later. Until I know what I’m doing with the data I won’t mess with it. To continue data exploration, I want to see how many unique actors there are across the board.\n\nunique(movies$Actor.1.Name) #491\n\nunique(movies$Actor.2.Name) #559\n\nCool, so we have a wide range of different actors! We’re still going to have some duplicates, so who are the most common/popular actors across both?\n\nhead(movies%>%\n  count(movies$Actor.1.Name)%>%\n  arrange(desc(n)), n=10)\n\n   movies$Actor.1.Name  n\n1         Keanu Reeves 27\n2         Adam Sandler 20\n3    Leonardo DiCaprio 17\n4          Roger Moore 17\n5         Sean Connery 17\n6       Pierce Brosnan 14\n7        Harrison Ford 13\n8          Johnny Depp 12\n9         Richard Gere 11\n10          Tom Cruise 11\n\n\n\nhead(movies%>%\n  count(movies$Actor.2.Name)%>%\n  arrange(desc(n)), n=10)\n\n   movies$Actor.2.Name  n\n1      Keira Knightley 14\n2    Reese Witherspoon 13\n3   Scarlett Johansson 13\n4           Emma Stone 12\n5        Julia Roberts 12\n6      Renee Zellweger 12\n7     Jennifer Aniston 11\n8    Jennifer Lawrence 10\n9       Julianne Moore 10\n10        Cameron Diaz  9\n\n\nWe have the incredible Keanu Reeves and Keria Knightly leading the actors with 27 and 14 movies each, respectively."
  },
  {
    "objectID": "tidytuesday_exercise.html#analysis",
    "href": "tidytuesday_exercise.html#analysis",
    "title": "Tidy Tuesday Exercise",
    "section": "Analysis",
    "text": "Analysis\nAlright, now that we’ve explored the data a bit, let’s get into the juicy stuff – looking at these age differences.\n\nggplot()+\n  geom_point(aes(x=Actor.1.Age, y=Actor.2.Age), data=movies)+\n  geom_smooth(aes(x=Actor.1.Age, y=Actor.2.Age), data=movies)+\n  theme_bw()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe good news is it seems relatively steady in terms of age gaps between the two actors. Does this age difference change over time?\n\nggplot()+\n  geom_point(aes(x=Release.Year, y=Age.Difference), data=movies)+\n  theme_bw()\n\n\n\n\nIt seems pretty hard to see trends since we have such a large number of movies released more recently. Let’s see how else we can visualize this.\nSince we have such a long time-frame, I’m going to create a dummy variable for decade of release.\n\nmovies.decade<-movies%>%\n  mutate(Decade = ifelse(Release.Year %in% c(1930:1939), \"1930\",\n                         ifelse(Release.Year %in% c(1940:1949), \"1940\",\n                                ifelse(Release.Year %in% c(1950:1959), \"1950\",\n                                       ifelse(Release.Year %in% c(1960:1969), \"1960\",\n                                              ifelse(Release.Year %in% c(1970:1979), \"1970\",\n                                                     ifelse(Release.Year %in% c(1980:1989), \"1980\",\n                                                            ifelse(Release.Year %in% c(1990:1999), \"1990\",\n                                                                   ifelse(Release.Year %in% c(2000:2009), \"2000\",\n                                                                          ifelse(Release.Year %in% c(2010:2019), \"2010\", \"2020\"))))))))))\n\nNext, I want to plot the Age Differences relative to Decade and see if we can notice any trends when the scales are standardized to a proportion to try and mitigate the influx of movie production in the recent years.\n\nggplot()+\n  geom_bar(aes(x=Age.Difference, fill=Decade), data=movies.decade, position = \"fill\", width = 2)+\n  theme_bw()\n\n\n\n\nA barchart such as this helps us see which decades have certain age gaps, and using a proportional approach helps mitigate the large number of more recent movies. However, we still run into the problem where the number of movies created in each decade influences how we read the chart. For example, those movies made in the 2020s don’t seem like a large impact although these recent movies have over 20 years age differences, and those from the 1930s are barely visible.\n\nmovies.age<-movies.decade%>%\n  mutate(age.diff = ifelse(Age.Difference %in% c(0:9), \"<10\",\n                            ifelse(Age.Difference %in% c(10:19), \"10-20\",\n                                    ifelse(Age.Difference %in% c(20:29), \"20+\",\n                                            ifelse(Age.Difference %in% c(30:39), \"30+\",\n                                                    ifelse(Age.Difference %in% c(40:49), \"40+\", \"50+\"))))))\n\n\nggplot()+\n  geom_bar(aes(x=Decade, fill=age.diff), data=movies.age, position  = position_fill(reverse = TRUE))+\n  theme_bw()\n\n\n\n\nSwitching our (in)dependent variables helped us see how the movies in each trended towards different age differences. Shockingly, 2020s had some drastic age differences of 20+ years. Maybe not shockingly, overall, the older movies tended to have more age-gap couples, specifically the 1940s and 50s with most of their respective movies having couples that had over a 20 year age gap.\nI’m curious to see if any specific directors are guilty of leading these movies, or if it’s an industry-wide concern. Because of the variation by decade and the limited longevity of people’s careers, I’m going to keep the decade consideration as a grouping with these directors.\n\nmovies.direct<-movies.decade%>%\n  group_by(Director, Decade)%>%\n  summarize(mean.diff = mean(Age.Difference))%>%\n  count(mean.diff)%>%\n  arrange(desc(mean.diff))\n\n`summarise()` has grouped output by 'Director'. You can override using the\n`.groups` argument.\n\nhead(movies.direct, n=10)\n\n# A tibble: 10 × 3\n# Groups:   Director [10]\n   Director       mean.diff     n\n   <chr>              <dbl> <int>\n 1 Hal Ashby           52       1\n 2 Katt Shea           42       1\n 3 Roger Michell       41.5     1\n 4 Jon Amiel           39       1\n 5 Irving Pichel       36       1\n 6 Jonathan Lynn       34       1\n 7 Sofia Coppola       34       1\n 8 Daniel Petrie       33       1\n 9 Jean Negulesco      32       1\n10 Phillip Noyce       31.5     1\n\n\nSo, I was able to ultimately do this by Director and by decade but not both. There’s definitely a way to do it (and probably very easily), but if I’m honest I’m very tired and would rather do it separately and call it a day.\nAnyways, we see that our top 10 directors have age gaps greater than 30 years, but they only directed one movie. The most our directors have in this dataset is 2 movies, so I think it’s safe to say a few directors aren’t exclusively responsible for these age-gap couples.\nOur last bit of exploration is getting a bit intense. Age gaps can be acceptable (or at least legal), between two consenting adults, but do we have any couples who we should call the police on? Per Romeo and Juliet laws, we’ll give a 5 year buffer.\n\nmovies%>%\n  filter(Actor.1.Age < 18 | Actor.2.Age <18, \n         Age.Difference > 5)\n\n                Movie.Name Release.Year     Director Age.Difference\n1               Poison Ivy         1992    Katt Shea             42\n2                   Lolita         1997  Adrian Lyne             32\n3 The Man Who Wasn't There         2001    Joel Coen             29\n4       Notes on a Scandal         2006 Richard Eyre             20\n5                The Crush         1993 Alan Shapiro             14\n6      Remember the Titans         2000   Boaz Yakin              7\n        Actor.1.Name Actor.1.Gender Actor.1.Birthdate Actor.1.Age\n1       Tom Skerritt            man        1933-08-25          59\n2       Jeremy Irons            man        1948-09-19          49\n3 Billy Bob Thornton            man        1955-08-04          46\n4     Andrew Simpson            man        1989-01-01          17\n5         Cary Elwes            man        1962-10-26          31\n6         Ryan Hurst            man        1976-06-19          24\n        Actor.2.Name Actor.2.Gender Actor.2.Birthdate Actor.2.Age\n1     Drew Barrymore          woman        1975-02-22          17\n2    Dominique Swain          woman        1980-08-12          17\n3 Scarlett Johansson          woman        1984-11-22          17\n4     Cate Blanchett          woman        1969-05-14          37\n5 Alicia Silverstone          woman        1976-10-04          17\n6      Kate Bosworth          woman        1983-01-02          17\n\n\nWe have 6 lovely movies that are questionable. And all of which were made pretty recently, a bit shocking. Our women are Hollywood IT girls: Drew Barrymore, Dominque Swain, Scarlett Johansson, and Kate Bosworth. Our men… are old (comparatively). We have three 20+ age differences, making these men in their 40s-50s as the love interest of 17 year olds. We also have Cate Blanchett (37) with Andrew Simpson (17) in Notes on a Scandal – which is fitting – and Cary Elwes and Alicia Silverstone in The Crush. While not appropriate, these are the premises of the movies."
  },
  {
    "objectID": "tidytuesday_exercise.html#conclusions",
    "href": "tidytuesday_exercise.html#conclusions",
    "title": "Tidy Tuesday Exercise",
    "section": "Conclusions",
    "text": "Conclusions\nThis was a fun and slightly scandalous first Tidy Tuesday for me! I’m intruiged to see what more seasoned participants do with this information, as I feel like there’s a lot of fun ways you could spin this data.\nUltimately, by my elementary findings, we can’t find an immediate rhyme or reason for these age gaps, or patterns among movies by Release Year, director, or specific actors. Of 1155 movies, only 3 really called legality into question which is slightly affirming; however, they were all filmed pretty recently (1990s+). We also didn’t look at those situations on the cusp, like an 18/19 year old with and older costar. With the given pop news, maybe I should have looked more into if Leonardo DiCaprio is mentioned anywhere. That might be a subject for another day."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Data Visualization Exercise - WNBA GOAT",
    "section": "",
    "text": "This is the image I’m trying to replicate. I found this in an article at FiveThirtyEight that describes Cynthia Cooper-Dykes’s domination of the court in the 1990-2000s. She played on the Houston Comets and led them to being one of the best WNBA teams every by composite ratings. However, an employee at FiveThirtyEight created their own measure for team performance called Elo. The ratings change after every game based on the winner’s pregame win probability, with more unexpected wins resulting in more points shifting from the loser’s rating to the winner’s. The author used this system to support their argument for the Houston Comets’ domination, attributed to Cooper-Dyke.\nThe datasetsupplied has four different Elo ratings for each game played between 1997 and 2019 season. Two scores are the away team and two are the home team. Likewise, two are their respective rankings before the game while the other two are the respective rankings after the game.\nThe graph nor the article describe if the pre- or post-Elo score is used. However, it can be assumed that one game’s pre-score is the previous game’s post score, so I will use the post score to track Elo rankings over time."
  },
  {
    "objectID": "visualization_exercise.html#read-in-the-data",
    "href": "visualization_exercise.html#read-in-the-data",
    "title": "Data Visualization Exercise - WNBA GOAT",
    "section": "Read in the Data",
    "text": "Read in the Data\n\nelo<-read_csv(\"C:/users/abbie/Desktop/MADA2023/WNBA-stats/wnba-team-elo-ratings.csv\")\n\nRows: 10488 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): date, team1, team2, name1, name2\ndbl (11): season, neutral, playoff, score1, score2, elo1_pre, elo2_pre, elo1...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(elo)\n\nRows: 10,488\nColumns: 16\n$ season    <dbl> 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, …\n$ date      <chr> \"10/10/2019\", \"10/10/2019\", \"10/8/2019\", \"10/8/2019\", \"10/6/…\n$ team1     <chr> \"WAS\", \"CON\", \"WAS\", \"CON\", \"WAS\", \"CON\", \"WAS\", \"CON\", \"WAS…\n$ team2     <chr> \"CON\", \"WAS\", \"CON\", \"WAS\", \"CON\", \"WAS\", \"CON\", \"WAS\", \"CON…\n$ name1     <chr> \"Washington Mystics\", \"Connecticut Sun\", \"Washington Mystics…\n$ name2     <chr> \"Connecticut Sun\", \"Washington Mystics\", \"Connecticut Sun\", …\n$ neutral   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ playoff   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ score1    <dbl> 89, 78, 86, 90, 94, 81, 87, 99, 95, 86, 94, 90, 75, 78, 92, …\n$ score2    <dbl> 78, 89, 90, 86, 81, 94, 99, 87, 86, 95, 90, 94, 92, 56, 75, …\n$ elo1_pre  <dbl> 1684, 1634, 1693, 1626, 1671, 1648, 1700, 1618, 1694, 1624, …\n$ elo2_pre  <dbl> 1634, 1684, 1626, 1693, 1648, 1671, 1618, 1700, 1624, 1694, …\n$ elo1_post <dbl> 1692, 1627, 1684, 1634, 1693, 1626, 1671, 1648, 1700, 1618, …\n$ elo2_post <dbl> 1627, 1692, 1634, 1684, 1626, 1693, 1648, 1671, 1618, 1700, …\n$ prob1     <dbl> 0.718, 0.282, 0.476, 0.524, 0.399, 0.601, 0.763, 0.237, 0.74…\n$ is_home1  <dbl> 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, …\n\n#get date to date form\nelo$date<-anydate(elo$date) #different length character strings weren't compatable with lubridate \n\n#filter for HOU\nelo_hou_raw<-elo%>%\n  filter(team1 == \"HOU\" | team2 == \"HOU\",\n         !duplicated(date),\n         date < \"2001-01-01\")%>%            #Sometimes there's a duplicate entry for the same game where the only difference is team1 and team2 are switched\n  arrange(date)%>%\n  mutate(game = row_number())             #Count the game number since that is the x axis for the graph\nglimpse(elo_hou_raw)\n\nRows: 138\nColumns: 17\n$ season    <dbl> 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, …\n$ date      <date> 1997-06-21, 1997-06-24, 1997-06-26, 1997-06-28, 1997-06-30,…\n$ team1     <chr> \"HOU\", \"HOU\", \"NYL\", \"HOU\", \"HOU\", \"NYL\", \"NYL\", \"PHO\", \"HOU…\n$ team2     <chr> \"CLE\", \"PHO\", \"HOU\", \"LVA\", \"LAS\", \"HOU\", \"HOU\", \"HOU\", \"SAC…\n$ name1     <chr> \"Houston Comets\", \"Houston Comets\", \"New York Liberty\", \"Hou…\n$ name2     <chr> \"Cleveland Rockers\", \"Phoenix Mercury\", \"Houston Comets\", \"U…\n$ neutral   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ playoff   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ score1    <dbl> 76, 72, 62, 76, 71, 72, 65, 69, 89, 82, 77, 86, 64, 81, 63, …\n$ score2    <dbl> 56, 55, 60, 58, 66, 67, 58, 64, 61, 60, 69, 76, 73, 57, 74, …\n$ elo1_pre  <dbl> 1500, 1530, 1539, 1535, 1558, 1556, 1564, 1554, 1547, 1542, …\n$ elo2_pre  <dbl> 1500, 1515, 1544, 1479, 1498, 1564, 1556, 1554, 1485, 1466, …\n$ elo1_post <dbl> 1530, 1544, 1548, 1558, 1564, 1564, 1579, 1561, 1563, 1567, …\n$ elo2_post <dbl> 1470, 1501, 1535, 1456, 1492, 1556, 1541, 1547, 1469, 1441, …\n$ prob1     <dbl> 0.387, 0.634, 0.381, 0.466, 0.692, 0.603, 0.397, 0.614, 0.69…\n$ is_home1  <dbl> 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, …\n$ game      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n\n\nNow we need to separate and rejoin the dataframes based on if HOU is team1 or team2 so we have a cohesive HOU dataframe regardless of home or away status.\n\n#Separate\nhou1<-elo_hou_raw%>%\n  filter(team1 == \"HOU\")%>%\n  select(season, date, team1, name1, elo1_post, game)%>%\n  rename(team = team1, name=name1, elo_post = elo1_post)\n\nhou2<-elo_hou_raw%>%\n  filter(team2 == \"HOU\")%>%\n  select(season, date, team2, name2, elo2_post, game)%>%\n  rename(team = team2, name=name2, elo_post = elo2_post)\n\n#Rejoin\nelo_hou_clean<-rbind(hou1, hou2)"
  },
  {
    "objectID": "visualization_exercise.html#making-the-plots",
    "href": "visualization_exercise.html#making-the-plots",
    "title": "Data Visualization Exercise - WNBA GOAT",
    "section": "Making the Plots",
    "text": "Making the Plots\n\nggplot()+\n  geom_line(aes(x=game, y=elo_post), data=elo_hou_clean, color = \"red\", linewidth =1)+\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_text(color = \"gray30\", family =  \"mono\", size = 12), \n        axis.text.x = element_text(color = \"gray30\", family =  \"mono\", size = 12), \n        plot.title = element_text(face = \"bold\", hjust = -.15, size = 15), \n        plot.subtitle = element_text(hjust = -.15, size = 12),\n        axis.title.x = element_text(face=\"bold\", family = \"sans\"),\n        axis.line.y = element_line(colour = \"black\", linewidth=.5, linetype = \"solid\"),\n        axis.line.x = element_line(colour = \"gray80\",linewidth=.5, linetype = \"solid\"), \n        plot.background = element_rect(fill = 'gray95'), panel.background = element_rect(fill = 'gray95'),\n        panel.grid.major = element_line(\"gray80\"),\n        panel.grid.minor = element_blank())+\n  labs(title = \"How Houston became the best WNBA team ever\", subtitle = \"Game-by-game Elo rating for the Houston Comets, 1997-2000\")+\n  scale_x_continuous(name=\"Game Number\", breaks = seq(0, 140, 20))+\n  scale_y_continuous(breaks = seq(1500, 1800, 100), limits = c(1500, 1830))\n\n\n\n\nThroughout this process, the biggest trouble I had was finding the correct fonts to match those of the articles. I referenced the R Cookbook a good bit during this step. I tried several fonts but could not find the exact one used by the authors - it may have been uploaded independently of the pre-loaded options. Mine are not off by a lot, but enough to notice. This exercise also allowed me to experiment more with the theme function and the different elements of the graph. In the past I have used many of these individually, but very rarely nearly all of the options. It makes my code seem a bit overwhelming, but the functionality is super nice!"
  }
]